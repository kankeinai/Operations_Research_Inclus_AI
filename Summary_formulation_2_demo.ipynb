{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from openai import AzureOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = AzureOpenAI(\n",
    "    api_key=\"b3f3d81d73ee4a979100ef674ac01dc4\", # DO NOT SHARE THIS IN CODE!\n",
    "    api_version=\"2024-02-15-preview\",\n",
    "    azure_endpoint=\"https://inclus-ai-rd.openai.azure.com/\" # DO NOT SHARE THIS IN CODE! Something like \"https://######.openai.azure.com/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"inclusData.xlsx\") # have the xlsx shared by Juha in the same folder as this .ipynb file\n",
    "df = df.sort_values(by=['Item', 'Dimension', 'Comment' ,'Answer'], na_position='last') # We can do some sorting beforehand, might be interesting to list the risk categories or comments with highest likelyhood first for example\n",
    "df_wide = df.pivot(index=['Item', 'Participant ID'], columns='Dimension', values=['Answer', 'Comment']) # likelihood & impact on the same row now; easier to deal with when looping by risk\n",
    "\n",
    "#%%\n",
    "# In this block we're parsing the data into a string that is fed into the prompt. Remember '\\t' for a tab and '\\n' for a line break.\n",
    "\n",
    "dimensions = df['Dimension'].unique() # likelihood & impact in this case\n",
    "\n",
    "# combine quant answers & comments into natural language\n",
    "for dimension in dimensions:\n",
    "    answer = df_wide[('Answer', dimension)].astype(str)\n",
    "    comment = df_wide[('Comment', dimension)] #.astype(str) # exclude .astype(str) if want to exclude 'nan' comments & their ratings from prompt\n",
    "    dimension_answer = ' ' # ' of this risk is '\n",
    "    answer_comment = '. Explanation: ' if dimension == 'Likelyhood' else '. Mitigation: ' #' because: ' if dimension == 'Likelyhood' else ' and it can be mitigated by: '\n",
    "    df_wide[('Rating and comment', dimension)] = dimension + dimension_answer + answer + answer_comment + comment # (Likelyhood, 4.0, reasons XYZ) |--> 'Likelyhood of this risk is 4.0 because: reasons XYZ'\n",
    "\n",
    "\n",
    "df = df.dropna()\n",
    "risk_data = ''\n",
    "for row in df.values:\n",
    "    risk_data += f'Risk type:  {row[4]}, Dimension: {row[5]}, Answer: {row[6]}, Comment: {row[7]} \\n' \n",
    "\n",
    "risk_aggregates = []\n",
    "col_names = ['Item']\n",
    "\n",
    "for dimension in dimensions:\n",
    "    col_names += ['Avg ' + dimension + ', all', 'Avg ' + dimension + ', given not nan comments', 'Number of comments, ' + dimension]\n",
    "\n",
    "col_names += ['likelihood x impact, all', 'likelihood x impact, given not nan comments']\n",
    "for item, group in df_wide.groupby('Item'):\n",
    "    means = [item]\n",
    "    for dimension in dimensions:\n",
    "        group_with_comment = group.dropna(subset=[('Comment', dimension)])\n",
    "        n_comments = group_with_comment.shape[0]\n",
    "        avg_given_comments = group_with_comment[('Answer', dimension)].mean()\n",
    "        avg = group[('Answer', dimension)].mean()\n",
    "        means += [avg, avg_given_comments, n_comments]\n",
    "    likelihood_x_impact = group[('Answer', 'Likelyhood')] * group[('Answer', 'Impact')]\n",
    "    gg = group.dropna(subset=[('Comment', 'Likelyhood'), ('Comment', 'Impact')])\n",
    "    likelihood_x_impact_given_comments = gg[('Answer', 'Likelyhood')] * gg[('Answer', 'Impact')]\n",
    "    means += [likelihood_x_impact.mean(), likelihood_x_impact_given_comments.mean()]\n",
    "    risk_aggregates.append( means )\n",
    "\n",
    "df_aggregate = pd.DataFrame(risk_aggregates, columns=col_names)\n",
    "df_aggregate['avg likelihood x avg impact, all'] = df_aggregate['Avg Impact, all'] * df_aggregate['Avg Likelyhood, all']\n",
    "df_aggregate['avg likelihood x avg impact, given not nan comments'] = df_aggregate['Avg Likelyhood, given not nan comments'] * df_aggregate['Avg Impact, given not nan comments']\n",
    "df_aggregate['Number of comments, combined'] = df_aggregate['Number of comments, Likelyhood'] + df_aggregate['Number of comments, Impact']\n",
    "df_aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we define the 'instruction' for the 'system' part for the prompt. Here the data 'risk_data' is also injected into the prompt, but it could be presented elsewhere.\n",
    "prompt_engineering_majick = 'The assistant is an expert in risk analyst.'\n",
    "task_context = 'The assistant will be asked questions about risk data (about the company Inclus) that is provided in this message and the assistant will try to answer using only the provided data. If assistant is not sure in the answer, it can return text \"None\". The assistant would be provided a list with indexes, risk type, dimension, answer for the dimension and comment. The number given in the answer tells about the impact of the dimension, 5 corresponds to the biggest impact and 1 corresponds to the smallest impact. It should answer on posed benchmark question. Do not add any additional text except for required output format. Assistant needs to take the impacts into account, when providing the analysis.'\n",
    "company_context = 'Inclus is a Finnish scaleup company that provides a platform for doing collaborative risk analysis.'\n",
    "data_context = 'The data that is provided below is gathered from multiple people within Inclus and concerns multiple risk events. Each risk event is described in a header above the individual assessments.'\n",
    "expected_output = 'HERE IS AN INSTRUCTION ON THE EXPECTED OUTPUT: Provide only several paragraphs of high-quality text that correctly describe important features of the data, such as significant risks and high-quality comments.'\n",
    "\n",
    "context_for_GPT = [prompt_engineering_majick,\n",
    "task_context,\n",
    "company_context,\n",
    "data_context,\n",
    "expected_output,\n",
    "risk_data]\n",
    "system_content = '\\n'.join(context_for_GPT) # goes into the prompt in 'messages' part in line '{\"role\": \"system\", \"content\": system_content}' later\n",
    "\n",
    "\n",
    "benchmark_question = 'Write an executive summary of the results of the risk analysis. Do not add any conclusion and comments or provide any additional data. The output should be solely based on the provided data. Use mainly the comments that are written by experts and that have a strong argument. For any conclusion made, provide all comments used in assesments as the source and write the sources underneath the summary. The amount of csources used should be at least 45.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_content},\n",
    "        {\"role\": \"user\", \"content\": benchmark_question} # the first strict benchmark question is asked\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.model_dump_json(indent=2))\n",
    "print(response.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
